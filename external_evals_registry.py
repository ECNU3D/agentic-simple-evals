"""
External Evaluations Registry System

This module manages the registration and discovery of external evaluations
generated by the Agentic Evaluation Builder, allowing them to be used
in simple_evals.py without modifying that file directly.
"""

import json
import os
import importlib.util
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional, Callable, Type
from dataclasses import dataclass, asdict
from eval_types import Eval, SamplerBase

REGISTRY_FILE = "external_evals_registry.json"

@dataclass
class ExternalEvalMetadata:
    """Metadata for an external evaluation"""
    name: str
    class_name: str
    file_path: str
    description: str
    task_type: str
    dataset_name: str
    requires_grader: bool = False
    requires_equality_checker: bool = False
    default_num_examples: Optional[int] = None
    supports_checkpointing: bool = True
    created_by: str = "agentic_builder"

class ExternalEvaluationRegistry:
    """Registry for managing external evaluations"""
    
    def __init__(self, registry_file: str = REGISTRY_FILE):
        self.registry_file = registry_file
        self.registry: Dict[str, ExternalEvalMetadata] = {}
        self.loaded_modules: Dict[str, Any] = {}
        self.load_registry()
    
    def load_registry(self):
        """Load the registry from file"""
        if os.path.exists(self.registry_file):
            try:
                with open(self.registry_file, 'r') as f:
                    data = json.load(f)
                self.registry = {
                    name: ExternalEvalMetadata(**metadata) 
                    for name, metadata in data.items()
                }
                print(f"✅ Loaded {len(self.registry)} external evaluations from registry")
            except Exception as e:
                print(f"⚠️  Warning: Could not load registry file: {e}")
                self.registry = {}
        else:
            print("📝 No external evaluations registry found - starting fresh")
            self.registry = {}
    
    def save_registry(self):
        """Save the registry to file"""
        try:
            data = {name: asdict(metadata) for name, metadata in self.registry.items()}
            with open(self.registry_file, 'w') as f:
                json.dump(data, f, indent=2)
            print(f"💾 Saved registry with {len(self.registry)} external evaluations")
        except Exception as e:
            print(f"❌ Error saving registry: {e}")
    
    def register_evaluation(self, metadata: ExternalEvalMetadata):
        """Register a new external evaluation"""
        if not os.path.exists(metadata.file_path):
            raise FileNotFoundError(f"Evaluation file not found: {metadata.file_path}")
        
        self.registry[metadata.name] = metadata
        self.save_registry()
        print(f"✅ Registered external evaluation: {metadata.name}")
    
    def unregister_evaluation(self, name: str):
        """Unregister an external evaluation"""
        if name in self.registry:
            del self.registry[name]
            self.save_registry()
            print(f"🗑️  Unregistered external evaluation: {name}")
        else:
            print(f"⚠️  Evaluation not found in registry: {name}")
    
    def list_evaluations(self) -> List[str]:
        """List all registered external evaluations"""
        return list(self.registry.keys())
    
    def get_metadata(self, name: str) -> Optional[ExternalEvalMetadata]:
        """Get metadata for a specific evaluation"""
        return self.registry.get(name)
    
    def load_evaluation_class(self, name: str) -> Optional[Type[Eval]]:
        """Dynamically load an evaluation class"""
        if name not in self.registry:
            return None
        
        metadata = self.registry[name]
        
        # Check if module is already loaded
        if name in self.loaded_modules:
            return getattr(self.loaded_modules[name], metadata.class_name)
        
        try:
            # Load the module dynamically
            spec = importlib.util.spec_from_file_location(
                f"external_eval_{name}", 
                metadata.file_path
            )
            if spec and spec.loader:
                module = importlib.util.module_from_spec(spec)
                sys.modules[f"external_eval_{name}"] = module
                spec.loader.exec_module(module)
                
                # Cache the loaded module
                self.loaded_modules[name] = module
                
                # Return the evaluation class
                eval_class = getattr(module, metadata.class_name)
                print(f"✅ Loaded external evaluation class: {metadata.class_name}")
                return eval_class
            else:
                print(f"❌ Could not create module spec for {metadata.file_path}")
                return None
                
        except Exception as e:
            print(f"❌ Error loading evaluation {name}: {e}")
            return None
    
    def create_evaluation_instance(self, name: str, **kwargs) -> Optional[Eval]:
        """Create an instance of an external evaluation"""
        eval_class = self.load_evaluation_class(name)
        if eval_class is None:
            return None
        
        metadata = self.registry[name]
        
        try:
            # Prepare initialization arguments based on metadata
            init_kwargs = {}
            
            # Handle common parameters
            if 'num_examples' in kwargs:
                init_kwargs['num_examples'] = kwargs['num_examples']
            elif metadata.default_num_examples is not None:
                init_kwargs['num_examples'] = metadata.default_num_examples
            
            if 'checkpoint_file' in kwargs and metadata.supports_checkpointing:
                init_kwargs['checkpoint_file'] = kwargs['checkpoint_file']
            
            # Handle special requirements
            if metadata.requires_grader and 'grader_model' in kwargs:
                init_kwargs['grader_model'] = kwargs['grader_model']
            
            if metadata.requires_equality_checker and 'equality_checker' in kwargs:
                init_kwargs['equality_checker'] = kwargs['equality_checker']
            
            # Pass through any other arguments
            for key, value in kwargs.items():
                if key not in init_kwargs:
                    init_kwargs[key] = value
            
            # Create the evaluation instance
            eval_instance = eval_class(**init_kwargs)
            print(f"✅ Created external evaluation instance: {name}")
            return eval_instance
            
        except Exception as e:
            print(f"❌ Error creating evaluation instance {name}: {e}")
            return None

# Global registry instance
_global_registry = None

def get_registry() -> ExternalEvaluationRegistry:
    """Get the global registry instance"""
    global _global_registry
    if _global_registry is None:
        _global_registry = ExternalEvaluationRegistry()
    return _global_registry

def register_external_evaluation(
    name: str,
    class_name: str, 
    file_path: str,
    description: str,
    task_type: str,
    dataset_name: str,
    **kwargs
):
    """Convenience function to register an external evaluation"""
    metadata = ExternalEvalMetadata(
        name=name,
        class_name=class_name,
        file_path=file_path,
        description=description,
        task_type=task_type,
        dataset_name=dataset_name,
        **kwargs
    )
    get_registry().register_evaluation(metadata)

def list_external_evaluations() -> List[str]:
    """List all registered external evaluations"""
    return get_registry().list_evaluations()

def load_external_evaluation(name: str, **kwargs) -> Optional[Eval]:
    """Load an external evaluation by name"""
    return get_registry().create_evaluation_instance(name, **kwargs)

# CLI functions for managing the registry
def main():
    """CLI interface for the registry"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Manage external evaluations registry")
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # List command
    list_parser = subparsers.add_parser('list', help='List all registered evaluations')
    
    # Register command
    register_parser = subparsers.add_parser('register', help='Register a new evaluation')
    register_parser.add_argument('name', help='Evaluation name')
    register_parser.add_argument('class_name', help='Python class name')
    register_parser.add_argument('file_path', help='Path to evaluation file')
    register_parser.add_argument('description', help='Description of the evaluation')
    register_parser.add_argument('task_type', help='Type of task')
    register_parser.add_argument('dataset_name', help='Source dataset name')
    
    # Unregister command
    unregister_parser = subparsers.add_parser('unregister', help='Unregister an evaluation')
    unregister_parser.add_argument('name', help='Evaluation name to remove')
    
    # Info command
    info_parser = subparsers.add_parser('info', help='Show info about an evaluation')
    info_parser.add_argument('name', help='Evaluation name')
    
    args = parser.parse_args()
    
    registry = get_registry()
    
    if args.command == 'list':
        evaluations = registry.list_evaluations()
        if evaluations:
            print("📋 Registered External Evaluations:")
            for name in evaluations:
                metadata = registry.get_metadata(name)
                print(f"  • {name} ({metadata.task_type}) - {metadata.description}")
        else:
            print("📝 No external evaluations registered")
    
    elif args.command == 'register':
        register_external_evaluation(
            name=args.name,
            class_name=args.class_name,
            file_path=args.file_path,
            description=args.description,
            task_type=args.task_type,
            dataset_name=args.dataset_name
        )
    
    elif args.command == 'unregister':
        registry.unregister_evaluation(args.name)
    
    elif args.command == 'info':
        metadata = registry.get_metadata(args.name)
        if metadata:
            print(f"📊 Evaluation: {metadata.name}")
            print(f"   Class: {metadata.class_name}")
            print(f"   File: {metadata.file_path}")
            print(f"   Description: {metadata.description}")
            print(f"   Task Type: {metadata.task_type}")
            print(f"   Dataset: {metadata.dataset_name}")
            print(f"   Requires Grader: {metadata.requires_grader}")
            print(f"   Requires Equality Checker: {metadata.requires_equality_checker}")
            print(f"   Supports Checkpointing: {metadata.supports_checkpointing}")
        else:
            print(f"❌ Evaluation not found: {args.name}")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main() 